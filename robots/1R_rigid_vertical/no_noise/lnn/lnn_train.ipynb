{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LNN"
      ],
      "metadata": {
        "id": "4dlBAEyodLjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental.ode import odeint\n",
        "from functools import partial\n",
        "\n",
        "# unconstrained equation of motion\n",
        "def unconstrained_eom(model, state, t=None):\n",
        "  q, q_t = jnp.split(state, 2)\n",
        "  return model(q, q_t)\n",
        "\n",
        "# lagrangian equation of motion\n",
        "def lagrangian_eom(lagrangian, state, t=None):\n",
        "  q, q_t = jnp.split(state, 2)\n",
        "  #Note: the following line assumes q is an angle. Delete it for problems other than double pendulum.\n",
        "  q = q % (2*jnp.pi)\n",
        "  q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n",
        "          @ (jax.grad(lagrangian, 0)(q, q_t)\n",
        "             - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n",
        "  dt = 1e-1\n",
        "  return dt*jnp.concatenate([q_t, q_tt])\n",
        "\n",
        "def raw_lagrangian_eom(lagrangian, state, t=None):\n",
        "  q, q_t = jnp.split(state, 2)\n",
        "  q = q % (2*jnp.pi)\n",
        "  q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n",
        "          @ (jax.grad(lagrangian, 0)(q, q_t)\n",
        "             - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n",
        "  return jnp.concatenate([q_t, q_tt])\n",
        "\n",
        "def lagrangian_eom_rk4(lagrangian, state, n_updates, Dt=1e-1, t=None):\n",
        "    @jax.jit\n",
        "    def cur_fnc(state):\n",
        "        q, q_t = jnp.split(state, 2)\n",
        "        q = q % (2*jnp.pi)\n",
        "        q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n",
        "                 @ (jax.grad(lagrangian, 0)(q, q_t)\n",
        "                 - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n",
        "        return jnp.concatenate([q_t, q_tt])\n",
        "\n",
        "    @jax.jit\n",
        "    def get_update(update):\n",
        "        dt = Dt/n_updates\n",
        "        cstate = state + update\n",
        "        k1 = dt*cur_fnc(cstate)\n",
        "        k2 = dt*cur_fnc(cstate + k1/2)\n",
        "        k3 = dt*cur_fnc(cstate + k2/2)\n",
        "        k4 = dt*cur_fnc(cstate + k3)\n",
        "        return update + 1.0/6.0 * (k1 + 2*k2 + 2*k3 + k4)\n",
        "\n",
        "    update = 0\n",
        "    for _ in range(n_updates):\n",
        "        update = get_update(update)\n",
        "    return update\n",
        "\n",
        "\n",
        "def solve_dynamics(dynamics_fn, initial_state, is_lagrangian=True, **kwargs):\n",
        "  eom = lagrangian_eom if is_lagrangian else unconstrained_eom\n",
        "\n",
        "  # We currently run odeint on CPUs only, because its cost is dominated by\n",
        "  # control flow, which is slow on GPUs.\n",
        "  @partial(jax.jit, backend='cpu')\n",
        "  def f(initial_state):\n",
        "    return odeint(partial(eom, dynamics_fn), initial_state, **kwargs)\n",
        "  return f(initial_state)\n",
        "\n",
        "\n",
        "def custom_init(init_params, seed=0):\n",
        "    \"\"\"Do an optimized LNN initialization for a simple uniform-width MLP\"\"\"\n",
        "    import numpy as np\n",
        "    new_params = []\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    i = 0\n",
        "    number_layers = len([0 for l1 in init_params if len(l1) != 0])\n",
        "    for l1 in init_params:\n",
        "        if (len(l1)) == 0: new_params.append(()); continue\n",
        "        new_l1 = []\n",
        "        for l2 in l1:\n",
        "            if len(l2.shape) == 1:\n",
        "                #Zero init biases\n",
        "                new_l1.append(jnp.zeros_like(l2))\n",
        "            else:\n",
        "                n = max(l2.shape)\n",
        "                first = int(i == 0)\n",
        "                last = int(i == number_layers - 1)\n",
        "                mid = int((i != 0) * (i != number_layers - 1))\n",
        "                mid *= i\n",
        "\n",
        "                std = 1.0/np.sqrt(n)\n",
        "                std *= 2.2*first + 0.58*mid + n*last\n",
        "\n",
        "                if std == 0:\n",
        "                    raise NotImplementedError(\"Wrong dimensions for MLP\")\n",
        "\n",
        "                new_l1.append(jax.random.normal(rng, l2.shape)*std)\n",
        "                rng += 1\n",
        "                i += 1\n",
        "\n",
        "        new_params.append(new_l1)\n",
        "\n",
        "    return new_params"
      ],
      "metadata": {
        "id": "fFWTbEPCdFpQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "-hTiSIu4d1Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import pickle\n",
        "\n",
        "def wrap_coords(state):\n",
        "  # wrap generalized coordinates to [-pi, pi]\n",
        "  return jnp.concatenate([(state[:2] + jnp.pi) % (2 * jnp.pi) - jnp.pi, state[2:]])\n",
        "\n",
        "def rk4_step(f, x, t, h):\n",
        "  # one step of Runge-Kutta integration\n",
        "  k1 = h * f(x, t)\n",
        "  k2 = h * f(x + k1/2, t + h/2)\n",
        "  k3 = h * f(x + k2/2, t + h/2)\n",
        "  k4 = h * f(x + k3, t + h)\n",
        "  return x + 1/6 * (k1 + 2 * k2 + 2 * k3 + k4)\n",
        "\n",
        "def radial2cartesian(t1, t2, l1, l2):\n",
        "  # Convert from radial to Cartesian coordinates.\n",
        "  x1 = l1 * jnp.sin(t1)\n",
        "  y1 = -l1 * jnp.cos(t1)\n",
        "  x2 = x1 + l2 * jnp.sin(t2)\n",
        "  y2 = y1 - l2 * jnp.cos(t2)\n",
        "  return x1, y1, x2, y2\n",
        "\n",
        "def write_to(data, path):\n",
        "  with open(path, 'wb') as f:\n",
        "    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def read_from(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "  return data"
      ],
      "metadata": {
        "id": "5D81fQLgdFW3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Physics"
      ],
      "metadata": {
        "id": "57OeKsfceAQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit\n",
        "\n",
        "@jit ### Usare solo t1,w1\n",
        "def kinetic_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8, d1=1,I=1):\n",
        "  (t1, t2), (w1, w2) = q, q_dot\n",
        "\n",
        "  T1 = 0.5 * (I + m1*d1**2)*w1**2\n",
        "  #T1 = 0.5 * m1 * (l1 * w1)**2\n",
        "  T2 = 0\n",
        "  T = T1 + T2\n",
        "  return T\n",
        "\n",
        "@jit\n",
        "def potential_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8,d1=1):\n",
        "  (t1, t2), (w1, w2) = q, q_dot\n",
        "\n",
        "  y1 = d1*jnp.sin(t1)\n",
        "  y2 = 0\n",
        "  V = m1 * g * y1 + m2 * g * y2\n",
        "  return V\n",
        "\n",
        "# Double pendulum lagrangian\n",
        "@jit\n",
        "def lagrangian_fn(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "  (t1, t2), (w1, w2) = q, q_dot\n",
        "\n",
        "  T = kinetic_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8)\n",
        "  V = potential_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8)\n",
        "  return T - V\n",
        "\n",
        "# Double pendulum lagrangian\n",
        "@jit\n",
        "def hamiltonian_fn(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "  (t1, t2), (w1, w2) = q, q_dot\n",
        "\n",
        "  T = kinetic_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8)\n",
        "  V = potential_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8)\n",
        "  return T + V\n",
        "\n",
        "\n",
        "# Double pendulum dynamics via analytical forces taken from Diego's blog\n",
        "@jit ################################## Inserire I nel ragionamento di DIego\n",
        "def analytical_fn(state, t=0, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "  t1, t2, w1, w2 = state\n",
        "  a1 = (l2 / l1) * (m2 / (m1 + m2)) * jnp.cos(t1 - t2)\n",
        "  a2 = (l1 / l2) * jnp.cos(t1 - t2)\n",
        "  f1 = -(l2 / l1) * (m2 / (m1 + m2)) * (w2**2) * jnp.sin(t1 - t2) - (g / l1) * jnp.sin(t1)\n",
        "  f2 = (l1 / l2) * (w1**2) * jnp.sin(t1 - t2) - (g / l2) * jnp.sin(t2)\n",
        "  g1 = (f1 - a1 * f2) / (1 - a1 * a2)\n",
        "  g2 = (f2 - a2 * f1) / (1 - a1 * a2)\n",
        "  return jnp.stack([w1, w2, g1, g2])\n"
      ],
      "metadata": {
        "id": "AGvdboD-eCfQ"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "ww1DfR68db4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "import numpy as np # get rid of this eventually\n",
        "from jax.experimental.ode import odeint\n",
        "from functools import partial # reduces arguments to function by making some subset implicit\n",
        "\n",
        "import os, sys\n",
        "#THIS_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "#PARENT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "#sys.path.append(PARENT_DIR)\n",
        "\n",
        "#HACK\n",
        "#from .physics import lagrangian_fn, analytical_fn\n",
        "\n",
        "\n",
        "\n",
        "@partial(jax.jit, backend='cpu')\n",
        "def get_trajectory(y0, times, use_lagrangian=False, **kwargs):\n",
        "  # frames = int(fps*(t_span[1]-t_span[0]))\n",
        "  # times = jnp.linspace(t_span[0], t_span[1], frames)\n",
        "  # y0 = np.array([3*np.pi/7, 3*np.pi/4, 0, 0], dtype=np.float32)\n",
        "  if use_lagrangian:\n",
        "    y = solve_dynamics(lagrangian_fn, y0, t=times, is_lagrangian=True, rtol=1e-10, atol=1e-10, **kwargs)\n",
        "  else:\n",
        "    y = odeint(analytical_fn, y0, t=times, rtol=1e-10, atol=1e-10, **kwargs)\n",
        "  return y\n",
        "\n",
        "@partial(jax.jit, backend='cpu')\n",
        "def get_trajectory_lagrangian(y0, times, **kwargs):\n",
        "  return solve_dynamics(lagrangian_fn, y0, t=times, is_lagrangian=True, rtol=1e-10, atol=1e-10, **kwargs)\n",
        "\n",
        "@partial(jax.jit, backend='cpu')\n",
        "def get_trajectory_analytic(y0, times, **kwargs):\n",
        "    return odeint(analytical_fn, y0, t=times, rtol=1e-10, atol=1e-10, **kwargs)\n",
        "\n",
        "def get_dataset(seed=0, samples=1, t_span=[0,2000], fps=1, test_split=0.5, **kwargs):\n",
        "    data = {'meta': locals()}\n",
        "\n",
        "    # randomly sample inputs\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    frames = int(fps*(t_span[1]-t_span[0]))\n",
        "    times = np.linspace(t_span[0], t_span[1], frames)\n",
        "    y0 = np.array([3*np.pi/7, 3*np.pi/4, 0, 0], dtype=np.float32) ################################## TODO avere solo 2 valori\n",
        "\n",
        "    xs, dxs = [], []\n",
        "    vfnc = jax.jit(jax.vmap(analytical_fn))\n",
        "    for s in range(samples):\n",
        "      x = get_trajectory(y0, times, **kwargs)\n",
        "      dx = vfnc(x)\n",
        "      xs.append(x) ; dxs.append(dx)\n",
        "\n",
        "    data['x'] = jax.vmap(wrap_coords)(jnp.concatenate(xs))\n",
        "    data['dx'] = jnp.concatenate(dxs)\n",
        "    data['t'] = times\n",
        "\n",
        "    # make a train/test split\n",
        "    split_ix = int(len(data['x']) * test_split)\n",
        "    split_data = {}\n",
        "    for k in ['x', 'dx', 't']:\n",
        "        split_data[k], split_data['test_' + k] = data[k][:split_ix], data[k][split_ix:]\n",
        "    data = split_data\n",
        "    return data"
      ],
      "metadata": {
        "id": "6sOyFVymdFhu"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "umAM16BueXwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "from jax.example_libraries import stax\n",
        "\n",
        "def mlp(args):\n",
        "    return stax.serial(\n",
        "        stax.Dense(args.hidden_dim),\n",
        "        stax.Softplus,\n",
        "        stax.Dense(args.hidden_dim),\n",
        "        stax.Softplus,\n",
        "        stax.Dense(args.output_dim),\n",
        "    )\n",
        "\n",
        "def pixel_encoder(args):\n",
        "    return stax.serial(\n",
        "        stax.Dense(args.ae_hidden_dim),\n",
        "        stax.Softplus,\n",
        "        stax.Dense(args.ae_latent_dim),\n",
        "    )\n",
        "\n",
        "def pixel_decoder(args):\n",
        "    return stax.serial(\n",
        "        stax.Dense(args.ae_hidden_dim),\n",
        "        stax.Softplus,\n",
        "        stax.Dense(args.ae_input_dim),\n",
        "    )"
      ],
      "metadata": {
        "id": "fpIG9arWeaki"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "8dDNk_2_ekw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np # get rid of this eventually\n",
        "import argparse\n",
        "from jax.experimental.ode import odeint\n",
        "from functools import partial # reduces arguments to function by making some subset implicit\n",
        "\n",
        "from jax.example_libraries import stax\n",
        "from jax.example_libraries import optimizers\n",
        "\n",
        "import os, sys, time\n",
        "#THIS_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "#PARENT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "#sys.path.append(PARENT_DIR)\n",
        "\n",
        "def get_args():\n",
        "    return {'input_dim': 4,\n",
        "           'hidden_dim': 128,\n",
        "           'output_dim': 4,\n",
        "           'dataset_size': 300,\n",
        "           'learn_rate': 1e-3,\n",
        "           'batch_size': 100,\n",
        "           'test_every': 10,\n",
        "           'num_batches': 500,\n",
        "           'name': 'dblpend',\n",
        "           'model': 'baseline_nn',\n",
        "           'verbose': True,\n",
        "           'seed': 1,\n",
        "           'save_dir': '.'}\n",
        "\n",
        "class ObjectView(object):\n",
        "    def __init__(self, d): self.__dict__ = d\n",
        "\n",
        "\n",
        "# replace the lagrangian with a parameteric model\n",
        "def learned_dynamics(params):\n",
        "  def dynamics(q, q_t):\n",
        "    assert q.shape == (2,)\n",
        "    state = wrap_coords(jnp.concatenate([q, q_t]))\n",
        "    return nn_forward_fn(params, state)\n",
        "  return dynamics\n",
        "\n",
        "@jax.jit\n",
        "def gln_loss(params, batch, time_step=None):\n",
        "  state, targets = batch\n",
        "  preds = jax.vmap(partial(lagrangian_eom, learned_dynamics(params)))(state)\n",
        "  return jnp.mean((preds - targets) ** 2)\n",
        "\n",
        "@jax.jit\n",
        "def baseline_loss(params, batch, time_step=None):\n",
        "  state, targets = batch\n",
        "  preds = jax.vmap(partial(unconstrained_eom, learned_dynamics(params)))(state)\n",
        "  return jnp.mean((preds - targets) ** 2)\n",
        "\n",
        "\n",
        "def train(args, model, data):\n",
        "  global opt_update, get_params, nn_forward_fn\n",
        "  (nn_forward_fn, init_params) = model\n",
        "  data = {k: jax.device_put(v) if type(v) is np.ndarray else v for k,v in data.items()}\n",
        "  time.sleep(2)\n",
        "\n",
        "  # choose our loss function\n",
        "  if args.model == 'gln':\n",
        "    loss = gln_loss\n",
        "  elif args.model == 'baseline_nn':\n",
        "    loss = baseline_loss\n",
        "  else:\n",
        "    raise(ValueError)\n",
        "\n",
        "  @jax.jit\n",
        "  def update_derivative(i, opt_state, batch):\n",
        "    params = get_params(opt_state)\n",
        "    return opt_update(i, jax.grad(loss)(params, batch, None), opt_state)\n",
        "\n",
        "  # make an optimizer\n",
        "  opt_init, opt_update, get_params = optimizers.adam(\n",
        "    lambda t: jnp.select([t < args.batch_size*(args.num_batches//3),\n",
        "                          t < args.batch_size*(2*args.num_batches//3),\n",
        "                          t > args.batch_size*(2*args.num_batches//3)],\n",
        "                         [args.learn_rate, args.learn_rate/10, args.learn_rate/100]))\n",
        "  opt_state = opt_init(init_params)\n",
        "\n",
        "  train_losses, test_losses = [], []\n",
        "  for iteration in range(args.batch_size*args.num_batches + 100000000):\n",
        "    if iteration % args.batch_size == 0:\n",
        "      params = get_params(opt_state)\n",
        "      train_loss = loss(params, (data['x'], data['dx']))\n",
        "      train_losses.append(train_loss)\n",
        "      test_loss = loss(params, (data['test_x'], data['test_dx']))\n",
        "      test_losses.append(test_loss)\n",
        "      if iteration % (args.batch_size*args.test_every) == 0:\n",
        "        print(f\"iteration={iteration}, train_loss={train_loss:.6f}, test_loss={test_loss:.6f}\")\n",
        "    opt_state = update_derivative(iteration, opt_state, (data['x'], data['dx']))\n",
        "\n",
        "  params = get_params(opt_state)\n",
        "  return params, train_losses, test_losses\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  args = ObjectView(get_args())\n",
        "  get_dataset(t_span=[0,args.dataset_size], fps=1, samples=1)\n",
        "\n",
        "  rng = jax.random.PRNGKey(args.seed)\n",
        "  init_random_params, nn_forward_fn = mlp(args)\n",
        "  _, init_params = init_random_params(rng, (-1, 4))\n",
        "  model = (nn_forward_fn, init_params)\n",
        "  data = get_dataset(t_span=[0,args.dataset_size], fps=1, samples=1)\n",
        "\n",
        "  result = train(args, model, data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "Hid7I-0yel2t",
        "outputId": "593d29de-574b-47c1-cc98-41db0a6d40b0"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration=0, train_loss=147.687943, test_loss=168.788727\n",
            "iteration=1000, train_loss=1.354874, test_loss=55.810055\n",
            "iteration=2000, train_loss=0.339376, test_loss=56.073711\n",
            "iteration=3000, train_loss=0.101268, test_loss=59.377411\n",
            "iteration=4000, train_loss=0.049351, test_loss=59.188881\n",
            "iteration=5000, train_loss=0.031396, test_loss=58.529095\n",
            "iteration=6000, train_loss=0.022752, test_loss=57.883633\n",
            "iteration=7000, train_loss=0.024463, test_loss=57.893948\n",
            "iteration=8000, train_loss=0.013814, test_loss=56.967461\n",
            "iteration=9000, train_loss=0.011376, test_loss=56.406246\n",
            "iteration=10000, train_loss=0.008368, test_loss=56.110340\n",
            "iteration=11000, train_loss=0.006379, test_loss=55.822117\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-1b6ff5260412>\u001b[0m in \u001b[0;36m<cell line: 101>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_span\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-100-1b6ff5260412>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, data)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_every\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"iteration={iteration}, train_loss={train_loss:.6f}, test_loss={test_loss:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/example_libraries/optimizers.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(data, xs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mOptimizerState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpacked_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtree_defs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     lambda data, xs: OptimizerState(xs[0], data[0], data[1]))  # type: ignore[index]\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}