{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dlBAEyodLjY"
      },
      "source": [
        "# LNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fFWTbEPCdFpQ"
      },
      "outputs": [],
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental.ode import odeint\n",
        "from functools import partial\n",
        "\n",
        "# unconstrained equation of motion\n",
        "def unconstrained_eom(model, state, t=None):\n",
        "  q, q_t = jnp.split(state, 2)\n",
        "  return model(q, q_t)\n",
        "\n",
        "# lagrangian equation of motion\n",
        "def lagrangian_eom(lagrangian, state, t=None):\n",
        "  q, q_t = jnp.split(state, 2)\n",
        "  #Note: the following line assumes q is an angle. Delete it for problems other than double pendulum.\n",
        "  q = q % (2*jnp.pi)\n",
        "  q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n",
        "          @ (jax.grad(lagrangian, 0)(q, q_t)\n",
        "             - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n",
        "  dt = 1e-1\n",
        "  return dt*jnp.concatenate([q_t, q_tt])\n",
        "\n",
        "def raw_lagrangian_eom(lagrangian, state, t=None):\n",
        "  q, q_t = jnp.split(state, 2)\n",
        "  q = q % (2*jnp.pi)\n",
        "  q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n",
        "          @ (jax.grad(lagrangian, 0)(q, q_t)\n",
        "             - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n",
        "  return jnp.concatenate([q_t, q_tt])\n",
        "\n",
        "def lagrangian_eom_rk4(lagrangian, state, n_updates, Dt=1e-1, t=None):\n",
        "    @jax.jit\n",
        "    def cur_fnc(state):\n",
        "        q, q_t = jnp.split(state, 2)\n",
        "        q = q % (2*jnp.pi)\n",
        "        q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n",
        "                 @ (jax.grad(lagrangian, 0)(q, q_t)\n",
        "                 - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n",
        "        return jnp.concatenate([q_t, q_tt])\n",
        "\n",
        "    @jax.jit\n",
        "    def get_update(update):\n",
        "        dt = Dt/n_updates\n",
        "        cstate = state + update\n",
        "        k1 = dt*cur_fnc(cstate)\n",
        "        k2 = dt*cur_fnc(cstate + k1/2)\n",
        "        k3 = dt*cur_fnc(cstate + k2/2)\n",
        "        k4 = dt*cur_fnc(cstate + k3)\n",
        "        return update + 1.0/6.0 * (k1 + 2*k2 + 2*k3 + k4)\n",
        "\n",
        "    update = 0\n",
        "    for _ in range(n_updates):\n",
        "        update = get_update(update)\n",
        "    return update\n",
        "\n",
        "\n",
        "def solve_dynamics(dynamics_fn, initial_state, is_lagrangian=True, **kwargs):\n",
        "  eom = lagrangian_eom if is_lagrangian else unconstrained_eom\n",
        "\n",
        "  # We currently run odeint on CPUs only, because its cost is dominated by\n",
        "  # control flow, which is slow on GPUs.\n",
        "  @partial(jax.jit, backend='cpu')\n",
        "  def f(initial_state):\n",
        "    return odeint(partial(eom, dynamics_fn), initial_state, **kwargs)\n",
        "  return f(initial_state)\n",
        "\n",
        "\n",
        "def custom_init(init_params, seed=0):\n",
        "    \"\"\"Do an optimized LNN initialization for a simple uniform-width MLP\"\"\"\n",
        "    import numpy as np\n",
        "    new_params = []\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    i = 0\n",
        "    number_layers = len([0 for l1 in init_params if len(l1) != 0])\n",
        "    for l1 in init_params:\n",
        "        if (len(l1)) == 0: new_params.append(()); continue\n",
        "        new_l1 = []\n",
        "        for l2 in l1:\n",
        "            if len(l2.shape) == 1:\n",
        "                #Zero init biases\n",
        "                new_l1.append(jnp.zeros_like(l2))\n",
        "            else:\n",
        "                n = max(l2.shape)\n",
        "                first = int(i == 0)\n",
        "                last = int(i == number_layers - 1)\n",
        "                mid = int((i != 0) * (i != number_layers - 1))\n",
        "                mid *= i\n",
        "\n",
        "                std = 1.0/np.sqrt(n)\n",
        "                std *= 2.2*first + 0.58*mid + n*last\n",
        "\n",
        "                if std == 0:\n",
        "                    raise NotImplementedError(\"Wrong dimensions for MLP\")\n",
        "\n",
        "                new_l1.append(jax.random.normal(rng, l2.shape)*std)\n",
        "                rng += 1\n",
        "                i += 1\n",
        "\n",
        "        new_params.append(new_l1)\n",
        "\n",
        "    return new_params\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hTiSIu4d1Rc"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5D81fQLgdFW3"
      },
      "outputs": [],
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import pickle\n",
        "\n",
        "def wrap_coords(state):\n",
        "  # wrap generalized coordinates to [-pi, pi]\n",
        "  return jnp.concatenate([(state[:2] + jnp.pi) % (2 * jnp.pi) - jnp.pi, state[2:]])\n",
        "\n",
        "def rk4_step(f, x, t, h):\n",
        "  # one step of Runge-Kutta integration\n",
        "  k1 = h * f(x, t)\n",
        "  k2 = h * f(x + k1/2, t + h/2)\n",
        "  k3 = h * f(x + k2/2, t + h/2)\n",
        "  k4 = h * f(x + k3, t + h)\n",
        "  return x + 1/6 * (k1 + 2 * k2 + 2 * k3 + k4)\n",
        "\n",
        "def radial2cartesian(t1, t2, l1, l2):\n",
        "  # Convert from radial to Cartesian coordinates.\n",
        "  x1 = l1 * jnp.sin(t1)\n",
        "  y1 = -l1 * jnp.cos(t1)\n",
        "  x2 = x1 + l2 * jnp.sin(t2)\n",
        "  y2 = y1 - l2 * jnp.cos(t2)\n",
        "  return x1, y1, x2, y2\n",
        "\n",
        "def write_to(data, path):\n",
        "  with open(path, 'wb') as f:\n",
        "    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def read_from(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6XRq_Upmcetp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57OeKsfceAQT"
      },
      "source": [
        "# Physics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AGvdboD-eCfQ"
      },
      "outputs": [],
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit\n",
        "\n",
        "@jit\n",
        "def kinetic_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "  (t1, t2), (w1, w2) = q, q_dot\n",
        "\n",
        "  T1 = 0.5 * m1 * (l1 * w1)**2\n",
        "  T2 = 0.5 * m2 * ((l1 * w1)**2 + (l2 * w2)**2 + 2 * l1 * l2 * w1 * w2 * jnp.cos(t1 - t2))\n",
        "  T = T1 + T2\n",
        "  return T\n",
        "\n",
        "@jit\n",
        "def potential_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "  (t1, t2), (w1, w2) = q, q_dot\n",
        "\n",
        "  y1 = -l1 * jnp.cos(t1)\n",
        "  y2 = y1 - l2 * jnp.cos(t2)\n",
        "  V = m1 * g * y1 + m2 * g * y2\n",
        "  return V\n",
        "\n",
        "# Double pendulum lagrangian\n",
        "@jit\n",
        "def lagrangian_fn(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "  (t1, t2), (w1, w2) = q, q_dot\n",
        "\n",
        "  T = kinetic_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8)\n",
        "  V = potential_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8)\n",
        "  return T - V\n",
        "\n",
        "# Double pendulum lagrangian\n",
        "@jit\n",
        "def hamiltonian_fn(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "  (t1, t2), (w1, w2) = q, q_dot\n",
        "\n",
        "  T = kinetic_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8)\n",
        "  V = potential_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8)\n",
        "  return T + V\n",
        "\n",
        "\n",
        "# Double pendulum dynamics via analytical forces taken from Diego's blog\n",
        "@jit\n",
        "def analytical_fn(state, t=0, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "  t1, t2, w1, w2 = state\n",
        "  a1 = (l2 / l1) * (m2 / (m1 + m2)) * jnp.cos(t1 - t2)\n",
        "  a2 = (l1 / l2) * jnp.cos(t1 - t2)\n",
        "  f1 = -(l2 / l1) * (m2 / (m1 + m2)) * (w2**2) * jnp.sin(t1 - t2) - (g / l1) * jnp.sin(t1)\n",
        "  f2 = (l1 / l2) * (w1**2) * jnp.sin(t1 - t2) - (g / l2) * jnp.sin(t2)\n",
        "  g1 = (f1 - a1 * f2) / (1 - a1 * a2)\n",
        "  g2 = (f2 - a2 * f1) / (1 - a1 * a2)\n",
        "  return jnp.stack([w1, w2, g1, g2])\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww1DfR68db4y"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6sOyFVymdFhu"
      },
      "outputs": [],
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "import numpy as np # get rid of this eventually\n",
        "from jax.experimental.ode import odeint\n",
        "from functools import partial # reduces arguments to function by making some subset implicit\n",
        "\n",
        "#import os, sys\n",
        "#THIS_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "#PARENT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "#sys.path.append(PARENT_DIR)\n",
        "\n",
        "#from lnn import solve_dynamics\n",
        "#from utils import wrap_coords\n",
        "#HACK\n",
        "#from .physics import lagrangian_fn, analytical_fn\n",
        "#from physics import lagrangian_fn, analytical_fn\n",
        "\n",
        "\n",
        "@partial(jax.jit, backend='cpu')\n",
        "def get_trajectory(y0, times, use_lagrangian=False, **kwargs):\n",
        "  # frames = int(fps*(t_span[1]-t_span[0]))\n",
        "  # times = jnp.linspace(t_span[0], t_span[1], frames)\n",
        "  # y0 = np.array([3*np.pi/7, 3*np.pi/4, 0, 0], dtype=np.float32)\n",
        "  if use_lagrangian:\n",
        "    y = solve_dynamics(lagrangian_fn, y0, t=times, is_lagrangian=True, rtol=1e-10, atol=1e-10, **kwargs)\n",
        "  else:\n",
        "    y = odeint(analytical_fn, y0, t=times, rtol=1e-10, atol=1e-10, **kwargs)\n",
        "  return y\n",
        "\n",
        "@partial(jax.jit, backend='cpu')\n",
        "def get_trajectory_lagrangian(y0, times, **kwargs):\n",
        "  return solve_dynamics(lagrangian_fn, y0, t=times, is_lagrangian=True, rtol=1e-10, atol=1e-10, **kwargs)\n",
        "\n",
        "@partial(jax.jit, backend='cpu')\n",
        "def get_trajectory_analytic(y0, times, **kwargs):\n",
        "    return odeint(analytical_fn, y0, t=times, rtol=1e-10, atol=1e-10, **kwargs)\n",
        "\n",
        "def get_dataset(seed=0, samples=1, t_span=[0,2000], fps=1, test_split=0.5, **kwargs):\n",
        "    data = {'meta': locals()}\n",
        "\n",
        "    # randomly sample inputs\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    frames = int(fps*(t_span[1]-t_span[0]))\n",
        "    times = np.linspace(t_span[0], t_span[1], frames)\n",
        "    y0 = np.array([3*np.pi/7, 3*np.pi/4, 0, 0], dtype=np.float32)\n",
        "\n",
        "    xs, dxs = [], []\n",
        "    vfnc = jax.jit(jax.vmap(analytical_fn))\n",
        "    for s in range(samples):\n",
        "      x = get_trajectory(y0, times, **kwargs)\n",
        "      dx = vfnc(x)\n",
        "      xs.append(x) ; dxs.append(dx)\n",
        "\n",
        "    data['x'] = jax.vmap(wrap_coords)(jnp.concatenate(xs))\n",
        "    data['dx'] = jnp.concatenate(dxs)\n",
        "    data['t'] = times\n",
        "\n",
        "    # make a train/test split\n",
        "    split_ix = int(len(data['x']) * test_split)\n",
        "    split_data = {}\n",
        "    for k in ['x', 'dx', 't']:\n",
        "        split_data[k], split_data['test_' + k] = data[k][:split_ix], data[k][split_ix:]\n",
        "    data = split_data\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umAM16BueXwf"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fpIG9arWeaki"
      },
      "outputs": [],
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "from jax.example_libraries import stax\n",
        "\n",
        "def mlp(args):\n",
        "    return stax.serial(\n",
        "        stax.Dense(args.hidden_dim),\n",
        "        stax.Softplus,\n",
        "        stax.Dense(args.hidden_dim),\n",
        "        stax.Softplus,\n",
        "        stax.Dense(args.output_dim),\n",
        "    )\n",
        "\n",
        "def pixel_encoder(args):\n",
        "    return stax.serial(\n",
        "        stax.Dense(args.ae_hidden_dim),\n",
        "        stax.Softplus,\n",
        "        stax.Dense(args.ae_latent_dim),\n",
        "    )\n",
        "\n",
        "def pixel_decoder(args):\n",
        "    return stax.serial(\n",
        "        stax.Dense(args.ae_hidden_dim),\n",
        "        stax.Softplus,\n",
        "        stax.Dense(args.ae_input_dim),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dDNk_2_ekw6"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "Hid7I-0yel2t",
        "outputId": "49c97397-9e5e-4592-955a-cc304b7ee87b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7b014e7fc939>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_span\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-7b014e7fc939>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, data)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m       \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m       \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_dx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-7b014e7fc939>\u001b[0m in \u001b[0;36mgln_loss\u001b[0;34m(params, batch, time_step)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgln_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlagrangian_eom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearned_dynamics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-0580b57c1334>\u001b[0m in \u001b[0;36mlagrangian_eom\u001b[0;34m(lagrangian, state, t)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n\u001b[0;32m---> 20\u001b[0;31m           @ (jax.grad(lagrangian, 0)(q, q_t)\n\u001b[0m\u001b[1;32m     21\u001b[0m              - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n\u001b[1;32m     22\u001b[0m   \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36m_check_scalar\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mShapedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"had shape: {aval.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"had abstract value {aval}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Gradient only defined for scalar-output functions. Output had shape: (4,)."
          ]
        }
      ],
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np # get rid of this eventually\n",
        "import argparse\n",
        "from jax.experimental.ode import odeint\n",
        "from functools import partial # reduces arguments to function by making some subset implicit\n",
        "\n",
        "from jax.example_libraries import stax\n",
        "from jax.example_libraries import optimizers\n",
        "\n",
        "#import os, sys, time\n",
        "#THIS_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "#PARENT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "#sys.path.append(PARENT_DIR)\n",
        "\n",
        "def get_args():\n",
        "    return {'input_dim': 4,\n",
        "           'hidden_dim': 128,\n",
        "           'output_dim': 4,\n",
        "           'dataset_size': 300,\n",
        "           'learn_rate': 1e-3,\n",
        "           'batch_size': 100,\n",
        "           'test_every': 10,\n",
        "           'num_batches': 500,\n",
        "           'name': 'dblpend',\n",
        "           'model': 'gln',\n",
        "           'verbose': True,\n",
        "           'seed': 1,\n",
        "           'save_dir': '.'}\n",
        "\n",
        "class ObjectView(object):\n",
        "    def __init__(self, d): self.__dict__ = d\n",
        "\n",
        "\n",
        "# replace the lagrangian with a parameteric model\n",
        "def learned_dynamics(params):\n",
        "  def dynamics(q, q_t):\n",
        "    assert q.shape == (2,)\n",
        "    state = wrap_coords(jnp.concatenate([q, q_t]))\n",
        "    return nn_forward_fn(params, state)\n",
        "  return dynamics\n",
        "\n",
        "@jax.jit\n",
        "def gln_loss(params, batch, time_step=None):\n",
        "  state, targets = batch\n",
        "  preds = jax.vmap(partial(lagrangian_eom, learned_dynamics(params)))(state)\n",
        "  return jnp.mean((preds - targets) ** 2)\n",
        "\n",
        "@jax.jit\n",
        "def baseline_loss(params, batch, time_step=None):\n",
        "  state, targets = batch\n",
        "  preds = jax.vmap(partial(unconstrained_eom, learned_dynamics(params)))(state)\n",
        "  return jnp.mean((preds - targets) ** 2)\n",
        "\n",
        "\n",
        "def train(args, model, data):\n",
        "  global opt_update, get_params, nn_forward_fn\n",
        "  (nn_forward_fn, init_params) = model\n",
        "  data = {k: jax.device_put(v) if type(v) is np.ndarray else v for k,v in data.items()}\n",
        "\n",
        "  # choose our loss function\n",
        "  if args.model == 'gln':\n",
        "    loss = gln_loss\n",
        "  elif args.model == 'baseline_nn':\n",
        "    loss = baseline_loss\n",
        "  else:\n",
        "    raise(ValueError)\n",
        "\n",
        "  @jax.jit\n",
        "  def update_derivative(i, opt_state, batch):\n",
        "    params = get_params(opt_state)\n",
        "    return opt_update(i, jax.grad(loss)(params, batch, None), opt_state)\n",
        "\n",
        "  # make an optimizer\n",
        "  opt_init, opt_update, get_params = optimizers.adam(\n",
        "    lambda t: jnp.select([t < args.batch_size*(args.num_batches//3),\n",
        "                          t < args.batch_size*(2*args.num_batches//3),\n",
        "                          t > args.batch_size*(2*args.num_batches//3)],\n",
        "                         [args.learn_rate, args.learn_rate/10, args.learn_rate/100]))\n",
        "  opt_state = opt_init(init_params)\n",
        "\n",
        "  train_losses, test_losses = [], []\n",
        "  for iteration in range(args.batch_size*args.num_batches + 1):\n",
        "    if iteration % args.batch_size == 0:\n",
        "      params = get_params(opt_state)\n",
        "      train_loss = loss(params, (data['x'], data['dx']))\n",
        "      train_losses.append(train_loss)\n",
        "      test_loss = loss(params, (data['test_x'], data['test_dx']))\n",
        "      test_losses.append(test_loss)\n",
        "      if iteration % (args.batch_size*args.test_every) == 0:\n",
        "        print(f\"iteration={iteration}, train_loss={train_loss:.6f}, test_loss={test_loss:.6f}\")\n",
        "    opt_state = update_derivative(iteration, opt_state, (data['x'], data['dx']))\n",
        "\n",
        "  params = get_params(opt_state)\n",
        "  return params, train_losses, test_losses\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  args = ObjectView(get_args())\n",
        "  get_dataset(t_span=[0,args.dataset_size], fps=1, samples=1)\n",
        "\n",
        "\n",
        "  rng = jax.random.PRNGKey(args.seed)\n",
        "  init_random_params, nn_forward_fn = mlp(args)\n",
        "  _, init_params = init_random_params(rng, (-1, 4))\n",
        "  model = (nn_forward_fn, init_params)\n",
        "  data = get_dataset(t_span=[0,args.dataset_size], fps=1, samples=1)\n",
        "\n",
        "  result = train(args, model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Train"
      ],
      "metadata": {
        "id": "8ZU4PL0wCbNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.tree_util import tree_flatten\n",
        "import numpy as np # get rid of this eventually\n",
        "import argparse\n",
        "from jax import jit\n",
        "from jax.experimental.ode import odeint\n",
        "from functools import partial # reduces arguments to function by making some subset implicit\n",
        "from jax.example_libraries import stax\n",
        "from jax.example_libraries import optimizers\n",
        "\n",
        "from jax.experimental.ode import odeint\n",
        "from jax.example_libraries.stax import serial, Dense, Softplus, Tanh, elementwise, Relu\n",
        "\n",
        "\n",
        "sigmoid = jit(lambda x: 1/(1+jnp.exp(-x)))\n",
        "swish = jit(lambda x: x/(1+jnp.exp(-x)))\n",
        "relu3 = jit(lambda x: jnp.clip(x, 0.0, float('inf'))**3)\n",
        "Swish = elementwise(swish)\n",
        "Relu3 = elementwise(relu3)\n",
        "\n",
        "vfnc = jax.jit(jax.vmap(analytical_fn))\n",
        "vget = partial(jax.jit, backend='cpu')(jax.vmap(partial(get_trajectory_analytic, mxstep=100), (0, None), 0))\n",
        "vget_unlimited = partial(jax.jit, backend='cpu')(jax.vmap(partial(get_trajectory_analytic), (0, None), 0))\n",
        "\n",
        "class ObjectView(object):\n",
        "    def __init__(self, d): self.__dict__ = d\n",
        "\n",
        "# replace the lagrangian with a parameteric model\n",
        "def learned_dynamics(params):\n",
        "  @jit\n",
        "  def dynamics(q, q_t):\n",
        "#     assert q.shape == (2,)\n",
        "    state = wrap_coords(jnp.concatenate([q, q_t]))\n",
        "    return jnp.squeeze(nn_forward_fn(params, state), axis=-1)\n",
        "  return dynamics\n",
        "\n",
        "def extended_mlp(args):\n",
        "    act = {\n",
        "        'softplus': [Softplus, Softplus],\n",
        "        'swish': [Swish, Swish],\n",
        "        'tanh': [Tanh, Tanh],\n",
        "        'tanh_relu': [Tanh, Relu],\n",
        "        'soft_relu': [Softplus, Relu],\n",
        "        'relu_relu': [Relu, Relu],\n",
        "        'relu_relu3': [Relu, Relu3],\n",
        "        'relu3_relu': [Relu3, Relu],\n",
        "        'relu_tanh': [Relu, Tanh],\n",
        "    }[args.act]\n",
        "    hidden = args.hidden_dim\n",
        "    output_dim = args.output_dim\n",
        "    nlayers = args.layers\n",
        "\n",
        "    layers = []\n",
        "    layers.extend([\n",
        "        Dense(hidden),\n",
        "        act[0]\n",
        "    ])\n",
        "    for _ in range(nlayers - 1):\n",
        "        layers.extend([\n",
        "            Dense(hidden),\n",
        "            act[1]\n",
        "        ])\n",
        "\n",
        "    layers.extend([Dense(output_dim)])\n",
        "\n",
        "    return stax.serial(*layers)\n",
        "\n",
        "\n",
        "def new_get_dataset(rng, samples=1, t_span=[0, 10], fps=100, test_split=0.5, lookahead=1,\n",
        "                    unlimited_steps=False, **kwargs):\n",
        "    data = {'meta': locals()}\n",
        "\n",
        "    # randomly sample inputs\n",
        "\n",
        "    frames = int(fps*(t_span[1]-t_span[0]))\n",
        "    times = jnp.linspace(t_span[0], t_span[1], frames)\n",
        "    y0 = jnp.concatenate([\n",
        "        jax.random.uniform(rng, (samples, 2))*2.0*np.pi,\n",
        "        jax.random.uniform(rng+1, (samples, 2))*0.1\n",
        "    ], axis=1)\n",
        "\n",
        "    #if not unlimited_steps:\n",
        "    #    y = vget(y0, times)\n",
        "    #else:\n",
        "    y = vget_unlimited(y0, times)\n",
        "\n",
        "    #This messes it up!\n",
        "#     y = np.concatenate(((y[..., :2]%(2*np.pi)) - np.pi, y[..., 2:]), axis=2)\n",
        "\n",
        "    data['x'] = y[:, :-lookahead]\n",
        "    data['dx'] = y[:, lookahead:] - data['x']\n",
        "    data['x'] = jnp.concatenate(data['x'])\n",
        "    data['dx'] = jnp.concatenate(data['dx'])\n",
        "    data['t'] = jnp.tile(times[:-lookahead], (samples,))\n",
        "\n",
        "    # make a train/test split\n",
        "    split_ix = int(len(data['x']) * test_split)\n",
        "    split_data = {}\n",
        "    for k in ['x', 'dx', 't']:\n",
        "        split_data[k], split_data['test_' + k] = data[k][:split_ix], data[k][split_ix:]\n",
        "    data = split_data\n",
        "    return data\n",
        "\n",
        "def make_loss(args):\n",
        "    if args.loss == 'l1':\n",
        "        @jax.jit\n",
        "        def gln_loss(params, batch, l2reg):\n",
        "            state, targets = batch#_rk4\n",
        "            leaves, _ = tree_flatten(params)\n",
        "            l2_norm = sum(jnp.vdot(param, param) for param in leaves)\n",
        "            preds = jax.vmap(partial(lagrangian_eom_rk4, learned_dynamics(params), Dt=args.dt, n_updates=args.n_updates))(state)\n",
        "            return jnp.sum(jnp.abs(preds - targets)) + l2reg*l2_norm/args.batch_size\n",
        "\n",
        "    else:\n",
        "        @jax.jit\n",
        "        def gln_loss(params, batch, l2reg):\n",
        "            state, targets = batch\n",
        "            preds = jax.vmap(partial(lagrangian_eom_rk4, learned_dynamics(params)))(state)\n",
        "            return jnp.sum(jnp.square(preds - targets)) + l2reg*l2_norm/args.batch_size\n",
        "\n",
        "\n",
        "    return gln_loss\n",
        "\n",
        "from copy import deepcopy as copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(args, model, data, rng):\n",
        "    global opt_update, get_params, nn_forward_fn\n",
        "    global best_params, best_loss\n",
        "    best_params = None\n",
        "    best_loss = np.inf\n",
        "    best_small_loss = np.inf\n",
        "    (nn_forward_fn, init_params) = model\n",
        "    data = {k: jax.device_put(v) for k,v in data.items()}\n",
        "\n",
        "    loss = make_loss(args)\n",
        "    opt_init, opt_update, get_params = optimizers.adam(\n",
        "    lambda t: jnp.select([t  < args.num_epochs//2,\n",
        "                          t >= args.num_epochs//2],\n",
        "                         [args.lr, args.lr2]))\n",
        "    opt_state = opt_init(init_params)\n",
        "\n",
        "    @jax.jit\n",
        "    def update_derivative(i, opt_state, batch, l2reg):\n",
        "        params = get_params(opt_state)\n",
        "        return opt_update(i, jax.grad(loss, 0)(params, batch, l2reg), opt_state), params\n",
        "\n",
        "    train_losses, test_losses = [], []\n",
        "\n",
        "    for iteration in range(args.num_epochs):\n",
        "        rand_idx = jax.random.randint(rng, (args.batch_size,), 0, len(data['x']))\n",
        "        rng += 1\n",
        "\n",
        "        batch = (data['x'][rand_idx], data['dx'][rand_idx])\n",
        "        opt_state, params = update_derivative(iteration, opt_state, batch, args.l2reg)\n",
        "        small_loss = loss(params, batch, 0.0)\n",
        "\n",
        "        #new_small_loss = False\n",
        "        #if small_loss < best_small_loss:\n",
        "        #    best_small_loss = small_loss\n",
        "        #    new_small_loss = True\n",
        "\n",
        "        #if new_small_loss or (iteration % 1000 == 0) or (iteration < 1000 and iteration % 100 == 0):\n",
        "        params = get_params(opt_state)\n",
        "        train_loss = loss(params, (data['x'], data['dx']), 0.0)/len(data['x'])\n",
        "        train_losses.append(train_loss)\n",
        "        test_loss = loss(params, (data['test_x'], data['test_dx']), 0.0)/len(data['test_x'])\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        #if test_loss < best_loss:\n",
        "        #    best_loss = test_loss\n",
        "        #    best_params = params\n",
        "\n",
        "        if jnp.isnan(test_loss).sum():\n",
        "            break\n",
        "\n",
        "        print(f\"iteration={iteration}, train_loss={train_loss:.6f}, test_loss={test_loss:.6f}\")\n",
        "\n",
        "    params = get_params(opt_state)\n",
        "    return params, train_losses, test_losses, best_loss\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "args = ObjectView(dict(\n",
        "    num_epochs=100, #40000\n",
        "    loss='l1',\n",
        "    l2reg=1e-6,\n",
        "    act='softplus',\n",
        "    hidden_dim=500,\n",
        "    output_dim=1,\n",
        "    dt=1e-1,\n",
        "    layers=4,\n",
        "    lr=1e-3*0.5,\n",
        "    lr2=1e-4*0.5,\n",
        "    model='gln',\n",
        "    n_updates=3,\n",
        "    batch_size=32,\n",
        "    fps=10,\n",
        "    samples=50,\n",
        "    dataset_size=100,\n",
        "))\n",
        "\n",
        "def test_args(args):\n",
        "  print('Running on', args.__dict__)\n",
        "  rng = jax.random.PRNGKey(0)\n",
        "  init_random_params, nn_forward_fn = extended_mlp(args)\n",
        "  _, init_params = init_random_params(rng+1, (-1, 4))\n",
        "  model = (nn_forward_fn, init_params)\n",
        "\n",
        "  data = new_get_dataset(jax.random.PRNGKey(0), t_span=[0, args.dataset_size], fps=args.fps, samples=args.samples, test_split=0.9)\n",
        "\n",
        "  result = train(args, model, data, rng+3)\n",
        "  print(result[3], 'is the loss for', args.__dict__)\n",
        "\n",
        "  if not jnp.isfinite(result[3]).sum():\n",
        "      return {'status': 'fail', 'loss': float('inf')}\n",
        "  return {'status': 'ok', 'loss': float(result[3])}\n",
        "\n",
        "test_args(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwr9g0gW8zbi",
        "outputId": "6d603465-6458-4421-d945-72e296611558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on {'num_epochs': 100, 'loss': 'l1', 'l2reg': 1e-06, 'act': 'softplus', 'hidden_dim': 500, 'output_dim': 1, 'dt': 0.1, 'layers': 4, 'lr': 0.0005, 'lr2': 5e-05, 'model': 'gln', 'n_updates': 3, 'batch_size': 32, 'fps': 10, 'samples': 50, 'dataset_size': 100}\n",
            "iteration=0, train_loss=7.833347, test_loss=7.631120\n",
            "iteration=1, train_loss=3.502684, test_loss=3.066644\n",
            "iteration=2, train_loss=3.053673, test_loss=2.594705\n",
            "iteration=3, train_loss=2.865578, test_loss=2.385074\n",
            "iteration=4, train_loss=2.768343, test_loss=2.274127\n",
            "iteration=5, train_loss=2.714701, test_loss=2.209445\n",
            "iteration=6, train_loss=2.695318, test_loss=2.186811\n",
            "iteration=7, train_loss=2.692868, test_loss=2.183981\n",
            "iteration=8, train_loss=2.697289, test_loss=2.188827\n",
            "iteration=9, train_loss=2.702371, test_loss=2.193837\n",
            "iteration=10, train_loss=2.709916, test_loss=2.201678\n",
            "iteration=11, train_loss=2.711080, test_loss=2.202115\n",
            "iteration=12, train_loss=2.711460, test_loss=2.202140\n",
            "iteration=13, train_loss=2.710099, test_loss=2.200434\n",
            "iteration=14, train_loss=2.710163, test_loss=2.200409\n",
            "iteration=15, train_loss=2.709274, test_loss=2.199526\n",
            "iteration=16, train_loss=2.706552, test_loss=2.196901\n",
            "iteration=17, train_loss=2.703398, test_loss=2.193974\n",
            "iteration=18, train_loss=2.698243, test_loss=2.189030\n",
            "iteration=19, train_loss=2.691465, test_loss=2.182975\n",
            "iteration=20, train_loss=2.687016, test_loss=2.179946\n",
            "iteration=21, train_loss=2.684364, test_loss=2.179323\n",
            "iteration=22, train_loss=2.682719, test_loss=2.180132\n",
            "iteration=23, train_loss=2.682290, test_loss=2.182379\n",
            "iteration=24, train_loss=2.683052, test_loss=2.186205\n",
            "iteration=25, train_loss=2.682529, test_loss=2.187200\n",
            "iteration=26, train_loss=2.681541, test_loss=2.186942\n",
            "iteration=27, train_loss=2.681615, test_loss=2.188285\n",
            "iteration=28, train_loss=2.679467, test_loss=2.185747\n",
            "iteration=29, train_loss=2.676373, test_loss=2.181624\n",
            "iteration=30, train_loss=2.672899, test_loss=2.176786\n",
            "iteration=31, train_loss=2.670398, test_loss=2.173604\n",
            "iteration=32, train_loss=2.668000, test_loss=2.170662\n",
            "iteration=33, train_loss=2.667574, test_loss=2.170586\n",
            "iteration=34, train_loss=2.667381, test_loss=2.171011\n",
            "iteration=35, train_loss=2.666155, test_loss=2.169788\n",
            "iteration=36, train_loss=2.665802, test_loss=2.169973\n",
            "iteration=37, train_loss=2.666054, test_loss=2.171034\n",
            "iteration=38, train_loss=2.664803, test_loss=2.169862\n",
            "iteration=39, train_loss=2.663806, test_loss=2.169091\n",
            "iteration=40, train_loss=2.661309, test_loss=2.166133\n",
            "iteration=41, train_loss=2.658609, test_loss=2.162821\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4dlBAEyodLjY",
        "-hTiSIu4d1Rc",
        "57OeKsfceAQT",
        "ww1DfR68db4y",
        "umAM16BueXwf",
        "8dDNk_2_ekw6"
      ],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}