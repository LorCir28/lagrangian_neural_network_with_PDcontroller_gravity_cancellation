{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Global"
      ],
      "metadata": {
        "id": "9OxARQx1SPVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sF-ufAySSCl",
        "outputId": "c37448aa-739e-4e86-9929-f099b6c2bb23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dlBAEyodLjY"
      },
      "source": [
        "# LNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFWTbEPCdFpQ"
      },
      "outputs": [],
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental.ode import odeint\n",
        "from functools import partial\n",
        "\n",
        "# unconstrained equation of motion\n",
        "def unconstrained_eom(model, state, t=None):\n",
        "  q, q_t = jnp.split(state, 2)\n",
        "  return model(q, q_t)\n",
        "\n",
        "# lagrangian equation of motion\n",
        "def lagrangian_eom(lagrangian, state, t=None):\n",
        "  q, q_t = jnp.split(state, 2)\n",
        "  #Note: the following line assumes q is an angle. Delete it for problems other than double pendulum.\n",
        "  q = q % (2*jnp.pi)\n",
        "  q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n",
        "          @ (jax.grad(lagrangian, 0)(q, q_t)\n",
        "             - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n",
        "  dt = 1e-1\n",
        "  return dt*jnp.concatenate([q_t, q_tt])\n",
        "\n",
        "def raw_lagrangian_eom(lagrangian, state, t=None):\n",
        "  q, q_t = jnp.split(state, 2)\n",
        "  q = q % (2*jnp.pi)\n",
        "  q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n",
        "          @ (jax.grad(lagrangian, 0)(q, q_t)\n",
        "             - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n",
        "  return jnp.concatenate([q_t, q_tt])\n",
        "\n",
        "def lagrangian_eom_rk4(lagrangian, state, n_updates, Dt=1e-1, t=None):\n",
        "    @jax.jit\n",
        "    def cur_fnc(state):\n",
        "        q, q_t = jnp.split(state, 2)\n",
        "        q = q % (2*jnp.pi)\n",
        "        q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n",
        "                 @ (jax.grad(lagrangian, 0)(q, q_t)\n",
        "                 - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n",
        "        return jnp.concatenate([q_t, q_tt])\n",
        "\n",
        "    @jax.jit\n",
        "    def get_update(update):\n",
        "        dt = Dt/n_updates\n",
        "        cstate = state + update\n",
        "        k1 = dt*cur_fnc(cstate)\n",
        "        k2 = dt*cur_fnc(cstate + k1/2)\n",
        "        k3 = dt*cur_fnc(cstate + k2/2)\n",
        "        k4 = dt*cur_fnc(cstate + k3)\n",
        "        return update + 1.0/6.0 * (k1 + 2*k2 + 2*k3 + k4)\n",
        "\n",
        "    update = 0\n",
        "    for _ in range(n_updates):\n",
        "        update = get_update(update)\n",
        "    return update\n",
        "\n",
        "\n",
        "def solve_dynamics(dynamics_fn, initial_state, is_lagrangian=True, **kwargs):\n",
        "  eom = lagrangian_eom if is_lagrangian else unconstrained_eom\n",
        "\n",
        "  # We currently run odeint on CPUs only, because its cost is dominated by\n",
        "  # control flow, which is slow on GPUs.\n",
        "  @partial(jax.jit, backend='cpu')\n",
        "  def f(initial_state):\n",
        "    return odeint(partial(eom, dynamics_fn), initial_state, **kwargs)\n",
        "  return f(initial_state)\n",
        "\n",
        "\n",
        "def custom_init(init_params, seed=0):\n",
        "    \"\"\"Do an optimized LNN initialization for a simple uniform-width MLP\"\"\"\n",
        "    import numpy as np\n",
        "    new_params = []\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    i = 0\n",
        "    number_layers = len([0 for l1 in init_params if len(l1) != 0])\n",
        "    for l1 in init_params:\n",
        "        if (len(l1)) == 0: new_params.append(()); continue\n",
        "        new_l1 = []\n",
        "        for l2 in l1:\n",
        "            if len(l2.shape) == 1:\n",
        "                #Zero init biases\n",
        "                new_l1.append(jnp.zeros_like(l2))\n",
        "            else:\n",
        "                n = max(l2.shape)\n",
        "                first = int(i == 0)\n",
        "                last = int(i == number_layers - 1)\n",
        "                mid = int((i != 0) * (i != number_layers - 1))\n",
        "                mid *= i\n",
        "\n",
        "                std = 1.0/np.sqrt(n)\n",
        "                std *= 2.2*first + 0.58*mid + n*last\n",
        "\n",
        "                if std == 0:\n",
        "                    raise NotImplementedError(\"Wrong dimensions for MLP\")\n",
        "\n",
        "                new_l1.append(jax.random.normal(rng, l2.shape)*std)\n",
        "                rng += 1\n",
        "                i += 1\n",
        "\n",
        "        new_params.append(new_l1)\n",
        "\n",
        "    return new_params\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hTiSIu4d1Rc"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D81fQLgdFW3"
      },
      "outputs": [],
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import pickle\n",
        "\n",
        "def wrap_coords(state):\n",
        "  # wrap generalized coordinates to [-pi, pi]\n",
        "  return jnp.concatenate([(state[:2] + jnp.pi) % (2 * jnp.pi) - jnp.pi, state[2:]])\n",
        "\n",
        "def rk4_step(f, x, t, h):\n",
        "  # one step of Runge-Kutta integration\n",
        "  k1 = h * f(x, t)\n",
        "  k2 = h * f(x + k1/2, t + h/2)\n",
        "  k3 = h * f(x + k2/2, t + h/2)\n",
        "  k4 = h * f(x + k3, t + h)\n",
        "  return x + 1/6 * (k1 + 2 * k2 + 2 * k3 + k4)\n",
        "\n",
        "def radial2cartesian(t1, t2, l1, l2):\n",
        "  # Convert from radial to Cartesian coordinates.\n",
        "  x1 = l1 * jnp.sin(t1)\n",
        "  y1 = -l1 * jnp.cos(t1)\n",
        "  x2 = x1 + l2 * jnp.sin(t2)\n",
        "  y2 = y1 - l2 * jnp.cos(t2)\n",
        "  return x1, y1, x2, y2\n",
        "\n",
        "def write_to(data, path):\n",
        "  with open(path, 'wb') as f:\n",
        "    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def read_from(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XRq_Upmcetp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57OeKsfceAQT"
      },
      "source": [
        "# Physics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGvdboD-eCfQ"
      },
      "outputs": [],
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit\n",
        "\n",
        "@jit\n",
        "def kinetic_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "  (t1, t2), (w1, w2) = q, q_dot\n",
        "\n",
        "  T1 = 0.5 * m1 * (l1 * w1)**2\n",
        "  T2 = 0.5 * m2 * ((l1 * w1)**2 + (l2 * w2)**2 + 2 * l1 * l2 * w1 * w2 * jnp.cos(t1 - t2))\n",
        "  T = T1 + T2\n",
        "  return T\n",
        "\n",
        "@jit\n",
        "def potential_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "  (t1, t2), (w1, w2) = q, q_dot\n",
        "\n",
        "  y1 = -l1 * jnp.cos(t1)\n",
        "  y2 = y1 - l2 * jnp.cos(t2)\n",
        "  V = m1 * g * y1 + m2 * g * y2\n",
        "  return V\n",
        "\n",
        "# Double pendulum lagrangian\n",
        "@jit\n",
        "def lagrangian_fn(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "  (t1, t2), (w1, w2) = q, q_dot\n",
        "\n",
        "  T = kinetic_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8)\n",
        "  V = potential_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8)\n",
        "  return T - V\n",
        "\n",
        "# Double pendulum lagrangian\n",
        "@jit\n",
        "def hamiltonian_fn(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "  (t1, t2), (w1, w2) = q, q_dot\n",
        "\n",
        "  T = kinetic_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8)\n",
        "  V = potential_energy(q, q_dot, m1=1, m2=1, l1=1, l2=1, g=9.8)\n",
        "  return T + V\n",
        "\n",
        "\n",
        "# Double pendulum dynamics via analytical forces taken from Diego's blog\n",
        "@jit\n",
        "def analytical_fn(state, t=0, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "  t1, t2, w1, w2 = state\n",
        "  a1 = (l2 / l1) * (m2 / (m1 + m2)) * jnp.cos(t1 - t2)\n",
        "  a2 = (l1 / l2) * jnp.cos(t1 - t2)\n",
        "  f1 = -(l2 / l1) * (m2 / (m1 + m2)) * (w2**2) * jnp.sin(t1 - t2) - (g / l1) * jnp.sin(t1)\n",
        "  f2 = (l1 / l2) * (w1**2) * jnp.sin(t1 - t2) - (g / l2) * jnp.sin(t2)\n",
        "  g1 = (f1 - a1 * f2) / (1 - a1 * a2)\n",
        "  g2 = (f2 - a2 * f1) / (1 - a1 * a2)\n",
        "  return jnp.stack([w1, w2, g1, g2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww1DfR68db4y"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sOyFVymdFhu"
      },
      "outputs": [],
      "source": [
        "# Generalized Lagrangian Networks | 2020\n",
        "# Miles Cranmer, Sam Greydanus, Stephan Hoyer (...)\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "import numpy as np # get rid of this eventually\n",
        "from jax.experimental.ode import odeint\n",
        "from functools import partial # reduces arguments to function by making some subset implicit\n",
        "\n",
        "#import os, sys\n",
        "#THIS_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "#PARENT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "#sys.path.append(PARENT_DIR)\n",
        "\n",
        "#from lnn import solve_dynamics\n",
        "#from utils import wrap_coords\n",
        "#HACK\n",
        "#from .physics import lagrangian_fn, analytical_fn\n",
        "#from physics import lagrangian_fn, analytical_fn\n",
        "\n",
        "\n",
        "@partial(jax.jit, backend='cpu')\n",
        "def get_trajectory(y0, times, use_lagrangian=False, **kwargs):\n",
        "  # frames = int(fps*(t_span[1]-t_span[0]))\n",
        "  # times = jnp.linspace(t_span[0], t_span[1], frames)\n",
        "  # y0 = np.array([3*np.pi/7, 3*np.pi/4, 0, 0], dtype=np.float32)\n",
        "  if use_lagrangian:\n",
        "    y = solve_dynamics(lagrangian_fn, y0, t=times, is_lagrangian=True, rtol=1e-10, atol=1e-10, **kwargs)\n",
        "  else:\n",
        "    y = odeint(analytical_fn, y0, t=times, rtol=1e-10, atol=1e-10, **kwargs)\n",
        "  return y\n",
        "\n",
        "@partial(jax.jit, backend='cpu')\n",
        "def get_trajectory_lagrangian(y0, times, **kwargs):\n",
        "  return solve_dynamics(lagrangian_fn, y0, t=times, is_lagrangian=True, rtol=1e-10, atol=1e-10, **kwargs)\n",
        "\n",
        "@partial(jax.jit, backend='cpu')\n",
        "def get_trajectory_analytic(y0, times, **kwargs):\n",
        "    return odeint(analytical_fn, y0, t=times, rtol=1e-10, atol=1e-10, **kwargs)\n",
        "\n",
        "def get_dataset(seed=0, samples=1, t_span=[0,2000], fps=1, test_split=0.5, **kwargs):\n",
        "    data = {'meta': locals()}\n",
        "\n",
        "    # randomly sample inputs\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    frames = int(fps*(t_span[1]-t_span[0]))\n",
        "    times = np.linspace(t_span[0], t_span[1], frames)\n",
        "    y0 = np.array([3*np.pi/7, 3*np.pi/4, 0, 0], dtype=np.float32)\n",
        "\n",
        "    xs, dxs = [], []\n",
        "    vfnc = jax.jit(jax.vmap(analytical_fn))\n",
        "    for s in range(samples):\n",
        "      x = get_trajectory(y0, times, **kwargs)\n",
        "      dx = vfnc(x)\n",
        "      xs.append(x) ; dxs.append(dx)\n",
        "\n",
        "    data['x'] = jax.vmap(wrap_coords)(jnp.concatenate(xs))\n",
        "    data['dx'] = jnp.concatenate(dxs)\n",
        "    data['t'] = times\n",
        "\n",
        "    # make a train/test split\n",
        "    split_ix = int(len(data['x']) * test_split)\n",
        "    split_data = {}\n",
        "    for k in ['x', 'dx', 't']:\n",
        "        split_data[k], split_data['test_' + k] = data[k][:split_ix], data[k][split_ix:]\n",
        "    data = split_data\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "8ZU4PL0wCbNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.tree_util import tree_flatten\n",
        "import numpy as np # get rid of this eventually\n",
        "import argparse\n",
        "from jax import jit\n",
        "from jax.experimental.ode import odeint\n",
        "from functools import partial # reduces arguments to function by making some subset implicit\n",
        "from jax.example_libraries import stax\n",
        "from jax.example_libraries import optimizers\n",
        "\n",
        "from jax.experimental.ode import odeint\n",
        "from jax.example_libraries.stax import serial, Dense, Softplus, Tanh, elementwise, Relu\n",
        "\n",
        "\n",
        "sigmoid = jit(lambda x: 1/(1+jnp.exp(-x)))\n",
        "swish = jit(lambda x: x/(1+jnp.exp(-x)))\n",
        "relu3 = jit(lambda x: jnp.clip(x, 0.0, float('inf'))**3)\n",
        "Swish = elementwise(swish)\n",
        "Relu3 = elementwise(relu3)\n",
        "\n",
        "vfnc = jax.jit(jax.vmap(analytical_fn))\n",
        "vget = partial(jax.jit, backend='cpu')(jax.vmap(partial(get_trajectory_analytic, mxstep=100), (0, None), 0))\n",
        "vget_unlimited = partial(jax.jit, backend='cpu')(jax.vmap(partial(get_trajectory_analytic), (0, None), 0))\n",
        "\n",
        "class ObjectView(object):\n",
        "    def __init__(self, d): self.__dict__ = d\n",
        "\n",
        "# replace the lagrangian with a parameteric model\n",
        "def learned_dynamics(params):\n",
        "  @jit\n",
        "  def dynamics(q, q_t):\n",
        "#     assert q.shape == (2,)\n",
        "    state = wrap_coords(jnp.concatenate([q, q_t]))\n",
        "    return jnp.squeeze(nn_forward_fn(params, state), axis=-1)\n",
        "  return dynamics\n",
        "\n",
        "def extended_mlp(args):\n",
        "    act = {\n",
        "        'softplus': [Softplus, Softplus],\n",
        "        'swish': [Swish, Swish],\n",
        "        'tanh': [Tanh, Tanh],\n",
        "        'tanh_relu': [Tanh, Relu],\n",
        "        'soft_relu': [Softplus, Relu],\n",
        "        'relu_relu': [Relu, Relu],\n",
        "        'relu_relu3': [Relu, Relu3],\n",
        "        'relu3_relu': [Relu3, Relu],\n",
        "        'relu_tanh': [Relu, Tanh],\n",
        "    }[args.act]\n",
        "    hidden = args.hidden_dim\n",
        "    output_dim = args.output_dim\n",
        "    nlayers = args.layers\n",
        "\n",
        "    layers = []\n",
        "    layers.extend([\n",
        "        Dense(hidden),\n",
        "        act[0]\n",
        "    ])\n",
        "    for _ in range(nlayers - 1):\n",
        "        layers.extend([\n",
        "            Dense(hidden),\n",
        "            act[1]\n",
        "        ])\n",
        "\n",
        "    layers.extend([Dense(output_dim)])\n",
        "\n",
        "    return stax.serial(*layers)\n",
        "\n",
        "\n",
        "def new_get_dataset(rng, samples=1, t_span=[0, 10], fps=100, test_split=0.5, lookahead=1,\n",
        "                    unlimited_steps=False, **kwargs):\n",
        "    data = {'meta': locals()}\n",
        "\n",
        "    # randomly sample inputs\n",
        "\n",
        "    frames = int(fps*(t_span[1]-t_span[0]))\n",
        "    times = jnp.linspace(t_span[0], t_span[1], frames)\n",
        "    y0 = jnp.concatenate([\n",
        "        jax.random.uniform(rng, (samples, 2))*2.0*np.pi,\n",
        "        jax.random.uniform(rng+1, (samples, 2))*0.1\n",
        "    ], axis=1)\n",
        "\n",
        "    #if not unlimited_steps:\n",
        "    #    y = vget(y0, times)\n",
        "    #else:\n",
        "    y = vget_unlimited(y0, times)\n",
        "\n",
        "    #This messes it up!\n",
        "#     y = np.concatenate(((y[..., :2]%(2*np.pi)) - np.pi, y[..., 2:]), axis=2)\n",
        "\n",
        "    data['x'] = y[:, :-lookahead]\n",
        "    data['dx'] = y[:, lookahead:] - data['x']\n",
        "    data['x'] = jnp.concatenate(data['x'])\n",
        "    data['dx'] = jnp.concatenate(data['dx'])\n",
        "    data['t'] = jnp.tile(times[:-lookahead], (samples,))\n",
        "\n",
        "    # make a train/test split\n",
        "    split_ix = int(len(data['x']) * test_split)\n",
        "    split_data = {}\n",
        "    for k in ['x', 'dx', 't']:\n",
        "        split_data[k], split_data['test_' + k] = data[k][:split_ix], data[k][split_ix:]\n",
        "    data = split_data\n",
        "    return data\n",
        "\n",
        "def make_loss(args):\n",
        "    if args.loss == 'l1':\n",
        "        @jax.jit\n",
        "        def gln_loss(params, batch, l2reg):\n",
        "            state, targets = batch#_rk4\n",
        "            leaves, _ = tree_flatten(params)\n",
        "            l2_norm = sum(jnp.vdot(param, param) for param in leaves)\n",
        "            preds = jax.vmap(partial(lagrangian_eom_rk4, learned_dynamics(params), Dt=args.dt, n_updates=args.n_updates))(state)\n",
        "            return jnp.sum(jnp.abs(preds - targets)) + l2reg*l2_norm/args.batch_size\n",
        "\n",
        "    else:\n",
        "        @jax.jit\n",
        "        def gln_loss(params, batch, l2reg):\n",
        "            state, targets = batch\n",
        "            preds = jax.vmap(partial(lagrangian_eom_rk4, learned_dynamics(params)))(state)\n",
        "            return jnp.sum(jnp.square(preds - targets)) + l2reg*l2_norm/args.batch_size\n",
        "\n",
        "\n",
        "    return gln_loss\n",
        "\n",
        "from copy import deepcopy as copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(args, model, data, rng):\n",
        "    global opt_update, get_params, nn_forward_fn\n",
        "    global best_params, best_loss\n",
        "    best_params = None\n",
        "    best_loss = np.inf\n",
        "    best_small_loss = np.inf\n",
        "    (nn_forward_fn, init_params) = model\n",
        "    data = {k: jax.device_put(v) for k,v in data.items()}\n",
        "\n",
        "    loss = make_loss(args)\n",
        "    opt_init, opt_update, get_params = optimizers.adam(\n",
        "    lambda t: jnp.select([t  < args.num_epochs//2,\n",
        "                          t >= args.num_epochs//2],\n",
        "                         [args.lr, args.lr2]))\n",
        "    opt_state = opt_init(init_params)\n",
        "\n",
        "    @jax.jit\n",
        "    def update_derivative(i, opt_state, batch, l2reg):\n",
        "        params = get_params(opt_state)\n",
        "        return opt_update(i, jax.grad(loss, 0)(params, batch, l2reg), opt_state), params\n",
        "\n",
        "    train_losses, test_losses = [], []\n",
        "\n",
        "    for iteration in range(args.num_epochs):\n",
        "        rand_idx = jax.random.randint(rng, (args.batch_size,), 0, len(data['x']))\n",
        "        rng += 1\n",
        "\n",
        "        batch = (data['x'][rand_idx], data['dx'][rand_idx])\n",
        "        opt_state, params = update_derivative(iteration, opt_state, batch, args.l2reg)\n",
        "        small_loss = loss(params, batch, 0.0)\n",
        "\n",
        "        #new_small_loss = False\n",
        "        #if small_loss < best_small_loss:\n",
        "        #    best_small_loss = small_loss\n",
        "        #    new_small_loss = True\n",
        "\n",
        "        #if new_small_loss or (iteration % 1000 == 0) or (iteration < 1000 and iteration % 100 == 0):\n",
        "        params = get_params(opt_state)\n",
        "        train_loss = loss(params, (data['x'], data['dx']), 0.0)/len(data['x'])\n",
        "        train_losses.append(train_loss)\n",
        "        test_loss = loss(params, (data['test_x'], data['test_dx']), 0.0)/len(data['test_x'])\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        #if test_loss < best_loss:\n",
        "        #    best_loss = test_loss\n",
        "        #    best_params = params\n",
        "\n",
        "        if jnp.isnan(test_loss).sum():\n",
        "            break\n",
        "\n",
        "        print(f\"iteration={iteration}, train_loss={train_loss:.6f}, test_loss={test_loss:.6f}\")\n",
        "\n",
        "    params = get_params(opt_state)\n",
        "    return params, train_losses, test_losses, best_loss\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "args = ObjectView(dict(\n",
        "    num_epochs=100, #40000\n",
        "    loss='l1',\n",
        "    l2reg=1e-6,\n",
        "    act='softplus',\n",
        "    hidden_dim=500,\n",
        "    output_dim=1,\n",
        "    dt=1e-1,\n",
        "    layers=4,\n",
        "    lr=1e-3*0.5,\n",
        "    lr2=1e-4*0.5,\n",
        "    model='gln',\n",
        "    n_updates=3,\n",
        "    batch_size=32,\n",
        "    fps=10,\n",
        "    samples=50,\n",
        "    dataset_size=100,\n",
        "))\n",
        "\n",
        "def test_args(args):\n",
        "  lnn_loaded = pkl.load(open('./LNN_params.pkl', 'rb'))\n",
        "\n",
        "test_args(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Xwr9g0gW8zbi",
        "outputId": "c62d1a77-1e88-42b3-c888-1a0ed56c0ff3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-73db7ea438a2>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mRelu3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melementwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mvfnc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalytical_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mvget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_trajectory_analytic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmxstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mvget_unlimited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_trajectory_analytic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'analytical_fn' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4dlBAEyodLjY",
        "-hTiSIu4d1Rc",
        "57OeKsfceAQT",
        "ww1DfR68db4y",
        "umAM16BueXwf",
        "8dDNk_2_ekw6"
      ],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}