{"cells":[{"cell_type":"markdown","metadata":{"id":"aSH3UDxeii7B"},"source":["# Global"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2646,"status":"ok","timestamp":1689490657996,"user":{"displayName":"claudio schiavella","userId":"11074666491842407425"},"user_tz":-120},"id":"LWPpOnpEilLB","outputId":"13a4a711-8a11-4746-eeb5-677a4d38fa23"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"4dlBAEyodLjY"},"source":["# LNN"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1689490657998,"user":{"displayName":"claudio schiavella","userId":"11074666491842407425"},"user_tz":-120},"id":"fFWTbEPCdFpQ"},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","from jax.experimental.ode import odeint\n","from functools import partial\n","\n","def unconstrained_eom(model, state, t=None):\n","  q, q_t = jnp.split(state, 2)\n","  return model(q, q_t)\n","\n","def lagrangian_eom(lagrangian, state, t=None):\n","  q, q_t = jnp.split(state, 2)\n","  q = q % (2*jnp.pi)\n","  q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n","          @ (jax.grad(lagrangian, 0)(q, q_t)\n","             - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n","  dt = 1e-1\n","  return dt*jnp.concatenate([q_t, q_tt])\n","\n","def raw_lagrangian_eom(lagrangian, state, t=None):\n","  q, q_t = jnp.split(state, 2)\n","  q = q % (2*jnp.pi)\n","  q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n","          @ (jax.grad(lagrangian, 0)(q, q_t)\n","             - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n","  return jnp.concatenate([q_t, q_tt])\n","\n","def lagrangian_eom_rk4(lagrangian, state, n_updates, Dt=1e-1, t=None):\n","    @jax.jit\n","    def cur_fnc(state):\n","        q, q_t = jnp.split(state, 2)\n","        q = q % (2*jnp.pi)\n","        q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n","                 @ (jax.grad(lagrangian, 0)(q, q_t)\n","                 - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n","        return jnp.concatenate([q_t, q_tt])\n","\n","    @jax.jit\n","    def get_update(update):\n","        dt = Dt/n_updates\n","        cstate = state + update\n","        k1 = dt*cur_fnc(cstate)\n","        k2 = dt*cur_fnc(cstate + k1/2)\n","        k3 = dt*cur_fnc(cstate + k2/2)\n","        k4 = dt*cur_fnc(cstate + k3)\n","        return update + 1.0/6.0 * (k1 + 2*k2 + 2*k3 + k4)\n","\n","    update = 0\n","    for _ in range(n_updates):\n","        update = get_update(update)\n","    return update\n","\n","\n","def solve_dynamics(dynamics_fn, initial_state, is_lagrangian=True, **kwargs):\n","  eom = lagrangian_eom if is_lagrangian else unconstrained_eom\n","\n","  @partial(jax.jit, backend='cpu')\n","  def f(initial_state):\n","    return odeint(partial(eom, dynamics_fn), initial_state, **kwargs)\n","  return f(initial_state)\n","\n","\n","def custom_init(init_params, seed=0):\n","    import numpy as np\n","    new_params = []\n","    rng = jax.random.PRNGKey(seed)\n","    i = 0\n","    number_layers = len([0 for l1 in init_params if len(l1) != 0])\n","    for l1 in init_params:\n","        if (len(l1)) == 0: new_params.append(()); continue\n","        new_l1 = []\n","        for l2 in l1:\n","            if len(l2.shape) == 1:\n","                new_l1.append(jnp.zeros_like(l2))\n","            else:\n","                n = max(l2.shape)\n","                first = int(i == 0)\n","                last = int(i == number_layers - 1)\n","                mid = int((i != 0) * (i != number_layers - 1))\n","                mid *= i\n","\n","                std = 1.0/np.sqrt(n)\n","                std *= 2.2*first + 0.58*mid + n*last\n","\n","                if std == 0:\n","                    raise NotImplementedError(\"Wrong dimensions for MLP\")\n","\n","                new_l1.append(jax.random.normal(rng, l2.shape)*std)\n","                rng += 1\n","                i += 1\n","\n","        new_params.append(new_l1)\n","\n","    return new_params\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-hTiSIu4d1Rc"},"source":["# Utils"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1689490657998,"user":{"displayName":"claudio schiavella","userId":"11074666491842407425"},"user_tz":-120},"id":"5D81fQLgdFW3"},"outputs":[],"source":["import jax.numpy as jnp\n","import pickle\n","\n","def wrap_coords(state):\n","  return jnp.concatenate([(state[:2] + jnp.pi) % (2 * jnp.pi) - jnp.pi, state[2:]])\n","\n","def rk4_step(f, x, t, h):\n","  k1 = h * f(x, t)\n","  k2 = h * f(x + k1/2, t + h/2)\n","  k3 = h * f(x + k2/2, t + h/2)\n","  k4 = h * f(x + k3, t + h)\n","  return x + 1/6 * (k1 + 2 * k2 + 2 * k3 + k4)\n","\n","def radial2cartesian(t1, t2, l1, l2):\n","  x1 = l1 * jnp.sin(t1)\n","  y1 = -l1 * jnp.cos(t1)\n","  x2 = x1 + l2 * jnp.sin(t2)\n","  y2 = y1 - l2 * jnp.cos(t2)\n","  return x1, y1, x2, y2\n","\n","def write_to(data, path):\n","  with open(path, 'wb') as f:\n","    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","def read_from(path):\n","  with open(path, 'rb') as f:\n","    data = pickle.load(f)\n","  return data"]},{"cell_type":"markdown","metadata":{"id":"57OeKsfceAQT"},"source":["# Physics"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1689490658000,"user":{"displayName":"claudio schiavella","userId":"11074666491842407425"},"user_tz":-120},"id":"AGvdboD-eCfQ"},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","from jax import jit\n","\n","@jit\n","def kinetic_energy(q, q_dot, m1=0.25, l1=0.30, g=9.81, I=0.01125, d1=0.15):\n","  t1, w1 = q, q_dot\n","  T = 0.5 * (I + m1*(d1**2))*(w1**2)\n","  return T\n","\n","@jit\n","def potential_energy(q, q_dot, m1=0.25, l1=0.30, g=9.81, I=0.01125, d1=0.15):\n","  t1, w1 = q, q_dot\n","  V = -m1*g*d1*jnp.cos(t1)\n","  return V\n","\n","@jit\n","def lagrangian_fn(q, q_dot, m1=0.25, l1=0.30, g=9.81, I=0.01125, d1=0.15):\n","  T = kinetic_energy(q, q_dot)\n","  V = potential_energy(q, q_dot)\n","  return jnp.sum(T - V)\n","\n","@jit\n","def analytical_fn(state, t=0, m1=0.25, l1=0.30, g=9.81, I=0.01125, d1=0.15):\n","  t1, w1 = state\n","  a1 = 0\n","  f1 = -m1*g*d1*jnp.sin(t1)/(I+m1*d1**2)\n","  g1 = f1\n","  return jnp.stack([w1,g1])"]},{"cell_type":"markdown","metadata":{"id":"ww1DfR68db4y"},"source":["# Data"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1689490662557,"user":{"displayName":"claudio schiavella","userId":"11074666491842407425"},"user_tz":-120},"id":"6sOyFVymdFhu"},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","from jax import random\n","import numpy as np\n","from jax.experimental.ode import odeint\n","from functools import partial\n","\n","@partial(jax.jit, backend='cpu')\n","def get_trajectory(y0, times, use_lagrangian=False, **kwargs):\n","  if use_lagrangian:\n","    y = solve_dynamics(lagrangian_fn, y0, t=times, is_lagrangian=True, rtol=1e-10, atol=1e-10, **kwargs)\n","  else:\n","    y = odeint(analytical_fn, y0, t=times, rtol=1e-10, atol=1e-10, **kwargs)\n","  return y\n","\n","@partial(jax.jit, backend='cpu')\n","def get_trajectory_lagrangian(y0, times, **kwargs):\n","  return solve_dynamics(lagrangian_fn, y0, t=times, is_lagrangian=True, rtol=1e-10, atol=1e-10, **kwargs)\n","\n","@partial(jax.jit, backend='cpu')\n","def get_trajectory_analytic(y0, times, **kwargs):\n","    return odeint(analytical_fn, y0, t=times, rtol=1e-10, atol=1e-10, **kwargs)\n","\n","vget_unlimited = partial(jax.jit, backend='cpu')(jax.vmap(partial(get_trajectory_analytic), (0, None), 0))\n","\n","def new_get_dataset(rng, samples=1, t_span=[0, 10], fps=100, test_split=0.5, lookahead=1,\n","                    unlimited_steps=False, **kwargs):\n","    data = {'meta': locals()}\n","\n","    frames = int(fps*(t_span[1]-t_span[0]))\n","    times = jnp.linspace(t_span[0], t_span[1], frames)\n","    y0 = jnp.concatenate([\n","        jax.random.uniform(rng, (samples, 1))*2.0*np.pi,\n","        jax.random.uniform(rng+1, (samples, 1))*0.1\n","    ], axis=1)\n","\n","    if unlimited_steps:\n","      y = vget(y0, times)\n","    else:\n","      y = vget_unlimited(y0, times)\n","\n","    data['x'] = y[:, :-lookahead]\n","    data['dx'] = y[:, lookahead:] - data['x']\n","    data['x'] = jnp.concatenate(data['x'])\n","    data['dx'] = jnp.concatenate(data['dx'])\n","    data['t'] = jnp.tile(times[:-lookahead], (samples,))\n","\n","    split_ix = int(len(data['x']) * test_split)\n","    split_data = {}\n","    for k in ['x', 'dx', 't']:\n","        split_data[k], split_data['test_' + k] = data[k][:split_ix], data[k][split_ix:]\n","    data = split_data\n","    return data"]},{"cell_type":"markdown","metadata":{"id":"umAM16BueXwf"},"source":["# Models"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":547,"status":"ok","timestamp":1689490665485,"user":{"displayName":"claudio schiavella","userId":"11074666491842407425"},"user_tz":-120},"id":"fpIG9arWeaki"},"outputs":[],"source":["from jax.example_libraries import stax\n","\n","def mlp(args):\n","    return stax.serial(\n","        stax.Dense(args.hidden_dim),\n","        stax.Softplus,\n","        stax.Dense(args.hidden_dim),\n","        stax.Softplus,\n","        stax.Dense(args.output_dim),\n","    )\n","\n","def pixel_encoder(args):\n","    return stax.serial(\n","        stax.Dense(args.ae_hidden_dim),\n","        stax.Softplus,\n","        stax.Dense(args.ae_latent_dim),\n","    )\n","\n","def pixel_decoder(args):\n","    return stax.serial(\n","        stax.Dense(args.ae_hidden_dim),\n","        stax.Softplus,\n","        stax.Dense(args.ae_input_dim),\n","    )"]},{"cell_type":"markdown","metadata":{"id":"8ZU4PL0wCbNq"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xwr9g0gW8zbi","outputId":"bd27a443-c213-42bf-92f6-a14986234fa9"},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","from jax.tree_util import tree_flatten\n","import numpy as np\n","import argparse\n","from jax import jit\n","from jax.experimental.ode import odeint\n","from functools import partial\n","from jax.example_libraries import stax\n","from jax.example_libraries import optimizers\n","\n","from jax.experimental.ode import odeint\n","from jax.example_libraries.stax import serial, Dense, Softplus, Tanh, elementwise, Relu\n","\n","\n","sigmoid = jit(lambda x: 1/(1+jnp.exp(-x)))\n","swish = jit(lambda x: x/(1+jnp.exp(-x)))\n","relu3 = jit(lambda x: jnp.clip(x, 0.0, float('inf'))**3)\n","Swish = elementwise(swish)\n","Relu3 = elementwise(relu3)\n","\n","vfnc = jax.jit(jax.vmap(analytical_fn))\n","vget = partial(jax.jit, backend='cpu')(jax.vmap(partial(get_trajectory_analytic, mxstep=100), (0, None), 0))\n","vget_unlimited = partial(jax.jit, backend='cpu')(jax.vmap(partial(get_trajectory_analytic), (0, None), 0))\n","\n","class ObjectView(object):\n","    def __init__(self, d): self.__dict__ = d\n","\n","def learned_dynamics(params):\n","  @jit\n","  def dynamics(q, q_t):\n","    state = wrap_coords(jnp.concatenate([q, q_t]))\n","    return jnp.squeeze(nn_forward_fn(params, state), axis=-1)\n","  return dynamics\n","\n","def extended_mlp(args):\n","    act = {\n","        'softplus': [Softplus, Softplus],\n","        'swish': [Swish, Swish],\n","        'tanh': [Tanh, Tanh],\n","        'tanh_relu': [Tanh, Relu],\n","        'soft_relu': [Softplus, Relu],\n","        'relu_relu': [Relu, Relu],\n","        'relu_relu3': [Relu, Relu3],\n","        'relu3_relu': [Relu3, Relu],\n","        'relu_tanh': [Relu, Tanh],\n","    }[args.act]\n","    hidden = args.hidden_dim\n","    output_dim = args.output_dim\n","    nlayers = args.layers\n","\n","    layers = []\n","    layers.extend([\n","        Dense(hidden),\n","        act[0]\n","    ])\n","    for _ in range(nlayers - 1):\n","        layers.extend([\n","            Dense(hidden),\n","            act[1]\n","        ])\n","\n","    layers.extend([Dense(output_dim)])\n","\n","    return stax.serial(*layers)\n","\n","\n","def make_loss(args):\n","    if args.loss == 'l1':\n","        @jax.jit\n","        def gln_loss(params, batch, l2reg):\n","            state, targets = batch\n","            leaves, _ = tree_flatten(params)\n","            l2_norm = sum(jnp.vdot(param, param) for param in leaves)\n","            preds = jax.vmap(partial(lagrangian_eom_rk4, learned_dynamics(params), Dt=args.dt, n_updates=args.n_updates))(state)\n","            return jnp.sum(jnp.abs(preds - targets)) + l2reg*l2_norm/args.batch_size\n","\n","    else:\n","        @jax.jit\n","        def gln_loss(params, batch, l2reg):\n","            state, targets = batch\n","            preds = jax.vmap(partial(lagrangian_eom_rk4, learned_dynamics(params)))(state)\n","            return jnp.sum(jnp.abs(preds - targets)) + l2reg/args.batch_size\n","\n","\n","    return gln_loss\n","\n","from copy import deepcopy as copy\n","from tqdm import tqdm\n","\n","def train(args, model, data, rng):\n","    global opt_update, get_params, nn_forward_fn\n","    global best_params, best_loss\n","    best_params = None\n","    best_loss = np.inf\n","    best_small_loss = np.inf\n","    (nn_forward_fn, init_params) = model\n","    data = {k: jax.device_put(v) for k,v in data.items()}\n","\n","    loss = make_loss(args)\n","    opt_init, opt_update, get_params = optimizers.adam(\n","    lambda t: jnp.select([t  < args.num_epochs//2,\n","                          t >= args.num_epochs//2],\n","                         [args.lr, args.lr2]))\n","    opt_state = opt_init(init_params)\n","\n","    @jax.jit\n","    def update_derivative(i, opt_state, batch, l2reg):\n","        params = get_params(opt_state)\n","        return opt_update(i, jax.grad(loss, 0)(params, batch, l2reg), opt_state), params\n","\n","    train_losses, test_losses = [], []\n","\n","    for iteration in range(args.num_epochs):\n","        rand_idx = jax.random.randint(rng, (args.batch_size,), 0, len(data['x']))\n","        rng += 1\n","\n","        batch = (data['x'][rand_idx], data['dx'][rand_idx])\n","        opt_state, params = update_derivative(iteration, opt_state, batch, args.l2reg)\n","        small_loss = loss(params, batch, 0.0)\n","\n","        new_small_loss = False\n","        if small_loss < best_small_loss:\n","            best_small_loss = small_loss\n","            new_small_loss = True\n","\n","        if new_small_loss or (iteration % 1000 == 0) or (iteration < 1000 and iteration % 100 == 0):\n","          params = get_params(opt_state)\n","          train_loss = loss(params, (data['x'], data['dx']), 0.0)/len(data['x'])\n","          train_losses.append(train_loss)\n","          test_loss = loss(params, (data['test_x'], data['test_dx']), 0.0)/len(data['test_x'])\n","          test_losses.append(test_loss)\n","\n","          if test_loss < best_loss:\n","              best_loss = test_loss\n","              best_params = params\n","\n","          if jnp.isnan(test_loss).sum():\n","              break\n","\n","          print(f\"-------> New best loss! Train_loss={train_loss:.6f}, Test_loss={test_loss:.6f}\")\n","        print(f\"Epoch: {iteration}/{args.num_epochs}\")\n","\n","    params = get_params(opt_state)\n","    return params, train_losses, test_losses, best_loss\n","\n","from matplotlib import pyplot as plt\n","\n","\n","args = ObjectView(dict(\n","    num_epochs=10000,\n","    loss='l1',\n","    l2reg=1e-6,\n","    act='softplus',\n","    hidden_dim=500,\n","    output_dim=1,\n","    dt=1e-1,\n","    layers=4,\n","    lr=1e-3*0.5,\n","    lr2=1e-4*0.5,\n","    model='gln',\n","    n_updates=3,\n","    batch_size=32,\n","    fps=10,\n","    samples=300,\n","    dataset_size=300,\n","))\n","\n","def test_args(args):\n","  print('Running on', args.__dict__)\n","  rng = jax.random.PRNGKey(0)\n","  init_random_params, nn_forward_fn = extended_mlp(args)\n","  _, init_params = init_random_params(rng+1, (-1, 2)) #4\n","  model = (nn_forward_fn, init_params)\n","\n","  data = new_get_dataset(jax.random.PRNGKey(0), t_span=[0, args.dataset_size], fps=args.fps, samples=args.samples, test_split=0.7)\n","\n","  result = train(args, model, data, rng+3)\n","  print(result[3], 'is the loss for', args.__dict__)\n","\n","  fig, ax = plt.subplots(1, 1)\n","\n","  ax.plot(result[1], label='Train loss')\n","  ax.plot(result[2], label='Test loss')\n","  ax.set_ylabel(\"Loss\")\n","  ax.set_xlabel(\"Updates\")\n","  ax.legend()\n","\n","  fig.tight_layout()\n","\n","  if not jnp.isfinite(result[3]).sum():\n","      return {'status': 'fail', 'loss': float('inf')}\n","\n","  pickle.dump({'params': best_params, 'args': args},open('/content/drive/MyDrive/Try_Robotics2_project/Rigid/LNN/lnn_rigid_params.pkl', 'wb'))\n","\n","  return {'status': 'ok', 'loss': float(result[3])}\n","\n","test_args(args)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN5RgcMe3l5Fgwqel3jQdVQ","collapsed_sections":["-hTiSIu4d1Rc","umAM16BueXwf"],"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
